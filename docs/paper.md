# MULTI-TASK LEARNING FOR RADAR SIGNAL CHARACTERISATION

Zi Huang<sup>⋆†</sup>, Akila Pemasiri<sup>⋆</sup>, Simon Denman<sup>⋆</sup>, Clinton Fookes<sup>⋆</sup>, Terrence Martin<sup>†</sup>  
<sup>⋆</sup>Queensland University of Technology, Australia  
<sup>†</sup>Revolution Aerospace, Australia

arXiv:2306.13105v2 [eess.SP] 30 Apr 2024

## ABSTRACT
Radio signal recognition is a crucial task in both civilian and military applications, as accurate and timely identification of unknown signals is an essential part of spectrum management and electronic warfare. The majority of research in this field has focused on applying deep learning for modulation classification, leaving the task of signal characterisation as an understudied area. This paper addresses this gap by presenting an approach for tackling radar signal classification and characterisation as a multi-task learning (MTL) problem. We propose the IQ Signal Transformer (IQST) among several reference architectures that allow for simultaneous optimisation of multiple regression and classification tasks. We demonstrate the performance of our proposed MTL model on a synthetic radar dataset, while also providing a first-of-its-kind benchmark for radar signal characterisation.

**Index Terms**— Multi-task Learning, Radio Signal Recognition, Radar Signal Characterisation, Automatic Modulation Classification, Radar Dataset, Transformer

## 1. INTRODUCTION
Recent innovations in deep learning (DL) coupled with the declining cost of computation have enabled the successful application of deep neural networks (DNNs) for radio signal recognition (RSR). RSR can be defined as the process of extracting the hidden characteristics within the radio frequency (RF) waveform to aid in the identification of unknown radio emitters. This capability is foundational to both civilian and military integrated sensing and communications (ISAC) applications [1], such as to improve the spectrum utilisation in communications networks and to enhance the spectrum situational awareness of soldiers in the modern battlefield.

Traditionally, classification of RF waveforms is achieved using likelihood-based [2] and feature-based [3, 4] methods that exploit the unique characteristics observed in the captured signal, such as its cyclostationary behaviour [5] and statistical features [4]. However, traditional methods are generally labour intensive requiring expert feature engineering and a priori knowledge about the signal characteristics, and thus cannot effectively cater for non-cooperative and covert spectrum users [1]. DL-based RSR solutions have garnered significant attention in recent years [6, 7] as they hold promise in effectively addressing these challenges.

The application of convolutional neural networks (CNNs) to automatic modulation classification (AMC) was introduced by [8]. Their early works [9, 10] together with the release of several public datasets [11] initiated a wave of interest in DL-based RSR. Recently, several alternative DL approaches that adopt recurrent neural networks (RNNs) and hybrid architectures [12] were able to consistently achieve above 90% modulation classification accuracy in relatively high signal-to-noise ratio (SNR) settings. Despite the success of DNNs, many recent approaches still rely on handcrafted features to pre-process the complex-valued, in-phase and quadrature (IQ) data into image-based representations, such as spectrograms [12], prior to training. These approaches effectively transform RSR into an image classification problem, and thus limits the ability of DNNs to extract the fine-grained temporal relationships within the IQ data. While transformers [13] have found success in adjacent fields such as audio signal processing [14], CNN-based models still dominate the current solution landscape for AMC.

Despite the progress made in RSR, the majority of recent research has only focused on AMC and wireless communication waveforms in a civilian context. While classifying modulation schemes can provide useful insight on the radio spectrum use, this information alone is insufficient in identifying or intercepting radio emitters, which is a highly desirable capability in a military context [1]. Signal characterisation extends the scope of AMC by extracting additional signal characteristics, such as estimating the pulse width (PW) and pulse repetition interval (PRI) of a radar transmission. Specifically for radar signal characterisation (RSC), estimating the pulse descriptor words (PDWs) of radar systems is an essential part of electronic warfare. PDWs, which comprise specific radar signal parameters such as PW and PRI, are essential for constructing threat libraries.

It is possible that limited DL research covering RSC may be attributed to the lack of publicly available radar datasets, as the majority of existing work in RSR has only focused on a single task, such as AMC [12]. Recently, multi-task learning (MTL) approaches to RSR were investigated in [15, 16]. These were the initial works that explored RSR as a joint problem by simultaneously classifying modulation and signal types on a synthetic radar and communication dataset [15]. While MTL was demonstrated in [16] to be effective at performing RSR in resource-constrained environments, this work was limited to classification tasks only. Furthermore, the proposed dataset lacks labelled signal characteristics that are required to support DL model development for RSC.

To address the existing gaps, this paper introduces a MTL framework for RSC. In addition, we introduce the IQ Signal Transformer (IQST) to perform automatic feature extraction on IQ data without requiring handcrafted features or image-based transforms. Our main contributions are threefold. First, we produce a synthetic radar signals dataset with multiple categorical and numerical labels needed to support MTL. Our dataset will be made available for public use[^1]. Second, we propose a novel MTL architecture for RSC solving classification and regression tasks as a joint problem. Finally, we introduce a new benchmark for RSC and provide several reference architectures for MTL.

[^1]: The download link to our synthetic dataset can be accessed via GitHub at: https://github.com/abcxyzi/RadChar

## 2. PROPOSED METHOD
### 2.1. Dataset Generation
Although existing datasets such as RadioML [11] and RadarComms [15] are useful for AMC, they do not provide training labels that are needed for RSC, and thus a new dataset is required. We generate our radar signals following the derivations in [17], at varying SNRs between -20 to 20 dB. To limit the number of signal parameters for the RSC problem, our dataset (RadChar) specifically focuses on pulsed radar signals. RadChar comprises a total of 5 radar signal types each covering 4 unique signal parameters. The signal classes include: Barker codes, polyphase Barker codes, Frank codes, linear frequency-modulated (LFM) pulses, and coherent unmodulated pulse trains. The signal parameters include PW (t<sub>pw</sub>), PRI (t<sub>pri</sub>), number of pulses (n<sub>p</sub>), and pulse time delay (t<sub>d</sub>). For phase-coded pulses, code lengths (l<sub>c</sub>) of up to 13 and 16 are considered in Barker and Frank codes respectively. A radar waveform example is shown in Figure 1.

We carefully design each waveform in RadChar to contain 512 baseband IQ samples (⃗x<sub>i</sub> + j⃗x<sub>q</sub>) while ensuring the range of radar parameter values used to construct the dataset adheres to the Nyquist-Shannon sampling theorem. The minimum sampling frequency (f<sub>s</sub>) required as a function of the selected radar characteristics is given by (1). The sampling rate used in RadChar is 3.2 MHz. The numerical bounds selected for radar parameters t<sub>pw</sub>, t<sub>pri</sub>, t<sub>d</sub> and n<sub>p</sub> are 10-16 µs, 17-23 µs, 1-10 µs, and 2-6 respectively. We apply uniformly random sampling across these value ranges for each signal class to generate 1 million unique radar waveforms. Additive white Gaussian noise (AWGN) is used to simulate varying SNRs in the dataset. In addition, we impose a unity average power to each radar waveform to ensure that signal power is scaled consistently across the dataset.

f<sub>s</sub> > 2 · max(l<sub>c</sub>·t<sub>pw</sub><sup>-1</sup>, t<sub>pri</sub><sup>-1</sup>, t<sub>d</sub><sup>-1</sup>) (1)

### 2.2. Multi-task Learning Framework
The proposed MTL model for RSC adopts the hard parameter shared MTL approach [18], where individual tasks share a single neural network backbone. Our model comprises two segments: a modular backbone for learning shared representations on the raw IQ data, and a set of parallel task-specific heads which consist of classification and regression tasks for signal classification and characterisation respectively. Our approach benefits from its modularity as the choice of the shared backbone is flexible allowing for domain adaptation, while additional task-specific heads can be added to increase the scope of the model. Furthermore, hard parameter sharing is advantageous for learning common representations of similar tasks, such as RSC tasks, and significantly reduces the risk of overfitting as the number of related tasks increases [19].

For the multi-task segment of our model, we follow the approach in [15] to construct task-specific heads using a minimal set of hidden layers. To achieve a lightweight design, all task-specific heads are the same depth and each contains a single convolutional layer with a kernal size of 3×3 followed by a dense layer. Dropout rates of 0.25 and 0.5 are applied to convolutional and dense layers respectively. The number of convolutional filters used here is driven by the output dimension of the shared backbone. We adopt the ReLU activation function in each head, while batch normalisation is applied prior to the activation function. Our model contains 5 task-specific heads which include a single classification head for signal classification, and 4 regression heads for signal characterisation. For classification, a softmax function is used to output probabilities for individual signal classes, while parameter predictions are obtained directly from the dense layer for each regression task.

The proposed MTL model is trained by optimising a compound multi-task loss (L<sub>mtl</sub>) function given by (2). The classification task is optimised using a categorical cross-entropy loss function, while the regression tasks are optimised using an L1 loss function. The multi-task loss is parameterised by shared parameters (θ<sub>sh</sub>) from the model backbone and task-specific parameters (θ<sub>1</sub>, ..., θ<sub>5</sub>) from individual task heads. The weights (w<sub>i</sub>) of task-specific losses are MTL hyperparameters and joint optimisation, given by (3), is achieved by minimising the total loss from task-specific heads.

L<sub>mtl</sub>(θ<sub>sh</sub>, θ<sub>1</sub>, ..., θ<sub>5</sub>) = Σ<sup>5</sup><sub>i=1</sub> w<sub>i</sub>L<sub>i</sub>(θ<sub>sh</sub>, θ<sub>i</sub>) (2)

argmin<sub>θ<sub>sh</sub>,θ<sub>1</sub>,...,θ<sub>5</sub></sub> L<sub>mtl</sub>(θ<sub>sh</sub>, θ<sub>1</sub>, ..., θ<sub>5</sub>) (3)

### 2.3. Shared Feature Extraction Backbones
We provide several reference designs of the MTL backbone to perform feature extraction on the raw IQ data. The uniqueness of our approach is that our models operate directly on raw IQ data requiring no additional pre-processing and feature transforms as seen in [6, 7]. Because our MTL architecture is inherently modular, our models can easily be extended to incorporate additional classification and regression tasks in order to increase the scope of RSC.

We provide two CNN implementations of the shared feature extraction backbone. CNN2D follows the same design philosophy as [15] to achieve a lightweight model. It comprises a single convolution layer with 8 filters using a kernal size of 2×2 followed by a 2×2 max pooling operation. Input to the CNN is a single channel 32×32 tensor which is reshaped from the raw IQ data. By intuition, such an uninformed reshaping operation is non-ideal for representing IQ data that is inherently sequential. We propose a modification to this approach by directly ingesting the IQ data as two separate I and Q channels of shape 2×512 to retain the shape of the raw IQ sequence. CNN1D uses 1D convolutional and max pooling operations instead, while maintaining the same number of filters as CNN2D. ReLU is used as the activation function in both backbones with a dropout rate of 0.25.

We introduce the IQ Signal Transformer (IQST), as shown in Figure 2, a novel attention-based architecture tailored for RSC and MTL. Our design is inspired by the Audio Spectrogram Transformer (AST) from [14], though adopts the standard transformer encoder architecture from [13]. Unlike [14], our approach operates on the raw signal allowing for direct feature extraction from the IQ data without the need to first transform the IQ data to an image representation. We adopt the patch embedding technique of [14, 20] to generate a sequence of 1D patch embeddings from a 2×512 tensor constructed from the raw IQ sequence. The dual-channel IQ data is flattened to form 8×1×128 blocks (or tokens) prior to applying a dense linear projection to form 8 learnable patch embeddings, each with an embedding dimension of 768. Each embedded patch is added to the standard positional embeddings from [13] to form a 128×8 input to the transformer encoder. We include an additional learnable embedding to the encoder to allow for common feature sharing across the individual tasks. This extra embedding is similar to the class embedding from [20]. The standard IQST (IQST-S) adopts the GELU activation function and implements 3 multi-head attention blocks and 3 encoder layers. We feed the outputs from the shared embedding as a 1×128 feature map into each task-specific head to complete the MTL model.

## 3. EXPERIMENTS
### 3.1. Training Details
We train and evaluate our models on a single Nvidia Tesla A100 GPU. A 70-15-15% train-validation-test split of RadChar is used for all our experiments. We train our models for 100 epochs with a learning rate of 5e-4 and a batch size of 64. We adopt the Adam optimiser and initialise the model parameters using LeCun initialisation. Importantly, we standardise the raw IQ samples against the training population mean and variance, and also normalise the regression labels between 0 and 1. The latter step significantly improves training performance and convergence for regression tasks, especially when dealing with small time values such as radar parameters.

### 3.2. MTL Model Performance
We evaluate our MTL models on the RadChar dataset. Model performance is compared on the same test set using task-specific metrics. Classification accuracy and mean absolute error (MAE) are selected to evaluate the performance of classification and regression tasks respectively. Table 1 provides a summary of individual task performance across various SNRs. The performance of a larger IQST (IQST-L), which uses 9 multi-head attention blocks and 6 encoder layers is also shown here for comparison. While CNN2D underperforms against other models across all tasks, IQST models generally perform better, especially at low SNRs. We observe from Figure 3 that as SNR increases, MAE decreases and classification accuracy improves, with the latter trend consistent with what is expected for AMC [8, 10].

| Model  | MAE(n<sub>p</sub>)  | MAE(t<sub>pw</sub>)   | MAE(t<sub>pri</sub>)  | MAE(t<sub>d</sub>)    | Class Acc.          |
|--------|---------------------|----------------------|----------------------|----------------------|---------------------|
| CNN1D  | 0.729, 0.193, 0.085 | 1.413, 0.560, 0.340  | 0.999, 0.330, 0.209  | 1.349, 0.385, 0.206  | 0.757, 0.998, 1.000 |
| CNN2D  | 0.793, 0.174, 0.090 | 1.466, 0.801, 0.505  | 1.054, 0.420, 0.299  | 1.729, 0.638, 0.443  | 0.673, 0.983, 0.998 |
| IQST-S | 0.733, 0.294, 0.251 | 1.282, 0.628, 0.364  | 0.816, 0.273, 0.192  | 1.229, 0.415, 0.277  | 0.792, 0.999, 1.000 |
| IQST-L | 0.752, 0.195, 0.124 | 1.253, 0.579, 0.334  | 0.799, 0.286, 0.225  | 1.253, 0.379, 0.233  | 0.791, 0.998, 1.000 |
*Table 1. Comparison of task performance across MTL models. Each value string (-,-,-) shows the performance of each task at -10, 0 and 10 dB SNR respectively. A lower MAE value is desired for regression, while a higher accuracy value indicates better classification performance. Note that the units for t<sub>pw</sub>, t<sub>pri</sub> and t<sub>d</sub> are expressed in µs.*

The outstanding performance of 1D models substantiates the importance of representing IQ data as 1D sequences. IQST benefits from its transformer architecture, which better captures longer-term dependencies between IQ samples, therefore allowing it to perform better at low SNRs. Although CNN1D outperforms IQST-S in the n<sub>p</sub> estimation task at high SNRs, increasing the model capacity, as in IQST-L, shows a significant improvement in task performance. While a larger transformer encoder is capable of capturing more complex dependencies in the IQ sequence, the computational cost is significantly increased due to its quadratic bottleneck [13]. Additionally, our results highlight the challenge in MTL, where a trade-off in task performance may need to be considered as the number of individual tasks increases. Nevertheless, our results indicate the potential for attention-based, hard parameter-shared MTL models for RSC.

### 3.3. Ablation Study
Increasing the number of convolutional layers did not provide a notable improvement on individual task performance. Instead, we find that deeper convolutional networks negatively impact regression tasks and result in higher errors. We hypothesise that regression tasks which require accurate estimation of time parameters are adversely affected by the stacking of operations, which reduces temporal resolution. Separately, selecting task weights that produce a relatively even distribution of w<sub>i</sub>L<sub>i</sub> during model initialisation provides stable task performance over all SNRs, while increasing the task weight to favour a specific task did not appear to help improve its test performance. This is true for both classification and regression tasks. Our observations are consistent with similar findings from [15] under the same SNR environment. A weight distribution of 0.1 for classification and 0.225 for all regression tasks was used in the experiments shown.

## 4. CONCLUSION
In this paper, we present a MTL framework for tackling RSC as a joint optimisation problem. We propose the IQST among other reference architectures to perform simultaneous optimisation of classification and regression tasks while highlighting the benefits of IQST for feature extraction on raw IQ data, particularly at low SNRs. We demonstrate the performance of our models on a synthetic radar dataset and provide a first-of-its-kind benchmark for RSC. The modularity of our proposed MTL design provides opportunities for additional classification and regression tasks in future work.

## 5. ACKNOWLEDGEMENT
The research for this paper received funding support from the Queensland Government through Trusted Autonomous Systems (TAS), a Defence Cooperative Research Centre funded through the Commonwealth Next Generation Technologies Fund and the Queensland Government.

## 6. REFERENCES
[1] Karen Haigh and Julia Andrusenko, *Cognitive Electronic Warfare: An Artificial Intelligence Approach*, Artech House, 2021.  
[2] Wen Wei and Jerry M. Mendel, “Maximum-Likelihood Classification for Digital Amplitude-Phase Modulations,” *IEEE Transactions on Communications*, vol. 48, no. 2, pp. 189–193, 2000.  
[3] Martin P. DeSimio and Glenn E. Prescott, “Adaptive Generation of Decision Functions for Classification of Digitally Modulated Signals,” in *Proceedings of the IEEE 1988 National Aerospace and Electronics Conference*. IEEE, 1988, pp. 1010–1014.  
[4] Hsiao-Chun Wu, Mohammad Saquib, and Zhifeng Yun, “Novel Automatic Modulation Classification Using Cumulant Features for Communications via Multipath Channels,” *IEEE Transactions on Wireless Communications*, vol. 7, no. 8, pp. 3098–3105, Aug. 2008.  
[5] Kyouwoong Kim, Ihsan A. Akbar, Kyung K. Bae, Jung-Sun Um, Chad M. Spooner, and Jeffrey H. Reed, “Cyclostationary Approaches to Signal Detection and Classification in Cognitive Radio,” in *2007 2nd IEEE International Symposium on New Frontiers in Dynamic Spectrum Access Networks*. IEEE, 2007, pp. 212–215.  
[6] Kyle Logue, Esteban Valles, Andres Vila, Alex Utter, Darren Semmen, Eugene Grayver, Sebastian Olsen, and Donna Branchevsky, “Expert RF Feature Extraction to Win the Army RCO AI Signal Classification Challenge,” in *Proceedings of the 18th Python in Science Conference*, 2019, pp. 8–14.  
[7] Andres Vila, Donna Branchevsky, Kyle Logue, Sebastian Olsen, Esteban Valles, Darren Semmen, Alex Utter, and Eugene Grayver, “Deep and Ensemble Learning to Win the Army RCO AI Signal Classification Challenge,” in *Proceedings of the 18th Python in Science Conference*, 2019, pp. 21–26.  
[8] Timothy J. O’Shea, Johnathan Corgan, and T. Charles Clancy, “Convolutional radio modulation recognition networks,” in *Engineering Applications of Neural Networks: 17th International Conference, EANN 2016, Aberdeen, UK, September 2-5, 2016, Proceedings 17*. 2016, pp. 213–226, Springer.  
[9] Timothy J. O’Shea, Johnathan Corgan, and T. Charles Clancy, “Unsupervised representation learning of structured radio communication signals,” in *2016 First International Workshop on Sensing, Processing and Learning for Intelligent Machines (SPLINE)*. 2016, pp. 1–5, IEEE.  
[10] Nathan E. West and Timothy J. O’shea, “Deep architectures for modulation recognition,” in *2017 IEEE International Symposium on Dynamic Spectrum Access Networks (DySPAN)*. 2017, pp. 1–6, IEEE.  
[11] Timothy J. O’Shea, Tamoghna Roy, and T. Charles Clancy, “Over-the-Air Deep Learning Based Radio Signal Classification,” *IEEE Journal of Selected Topics in Signal Processing*, vol. 12, no. 1, pp. 168–179, 2018, IEEE.  
[12] Thien Huynh-The, Quoc-Viet Pham, Toan-Van Nguyen, Thanh Thi Nguyen, Rukhsana Ruby, Ming Zeng, and Dong-Seong Kim, “Automatic Modulation Classification: A Deep Architecture Survey,” *IEEE Access*, vol. 9, pp. 142950–142971, 2021.  
[13] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin, “Attention Is All You need,” *Advances in Neural Information Processing Systems*, vol. 30, 2017.  
[14] Yuan Gong, Yu-An Chung, and James Glass, “AST: Audio Spectrogram Transformer,” in *Proceedings of Interspeech 2021*, 2021, pp. 571–575.  
[15] Anu Jagannath and Jithin Jagannath, “Multi-task Learning Approach for Automatic Modulation and Wireless Signal Classification,” in *ICC 2021-IEEE International Conference on Communications*. 2021, pp. 1–7, IEEE.  
[16] Anu Jagannath and Jithin Jagannath, “Multi-task Learning Approach for Modulation and Wireless Signal Classification for 5G and Beyond: Edge Deployment via Model Compression,” *Physical Communication*, vol. 54, pp. 101793, 2022, Elsevier.  
[17] Nadav Levanon and Eli Mozeson, *Radar Signals*, John Wiley & Sons, 2004.  
[18] Richard A. Caruana, “Multitask Learning: A Knowledge-Based Source of Inductive Bias,” in *Proceedings of the Tenth International Conference on Machine Learning*. 1993, pp. 41–48, Citeseer.  
[19] Jonathan Baxter, “A Bayesian/Information Theoretic Model of Learning to Learn via Multiple Task Sampling,” *Machine learning*, vol. 28, pp. 7–39, 1997.  
[20] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby, “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,” *ICLR 2021 - Ninth International Conference on Learning Representations*, 2021.

## Figures
*The following figures are referenced in the text. Please refer to the 'tables and figures' folder for the actual image files.*

*   **Fig. 1.** Radar signals sampled from the RadChar dataset illustrating polyphase Barker codes at varying SNRs.  
    (a) SNR of 20 dB  
    (b) SNR of 12 dB  
    (c) SNR of 4 dB
*   **Fig. 2.** The proposed hard parameter shared MTL architecture for RSC. This model shows an IQST backbone with task-specific classification and regression heads.
*   **Fig. 3.** Test performance of MTL models across an SNR range of -20 to 20 dB. MAE results of the regression tasks are shown in (a) to (d), and signal type classification accuracy is shown in (e).